<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Recent Content on Motoki Wu </title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://localhost:1313/index.xml</link>
    <language>en-us</language>
    
    
    <updated>Tue, 29 Jul 2014 00:00:00 UTC</updated>
    
    <item>
      <title>SMOTE Algorithm to Classify Imbalanced Datasets with Random Forests</title>
      <link>http://localhost:1313/posts/imbalanced-datasets-random-forests</link>
      <pubDate>Tue, 29 Jul 2014 00:00:00 UTC</pubDate>
      
      <guid>http://localhost:1313/posts/imbalanced-datasets-random-forests</guid>
      <description>

&lt;p&gt;Imbalanced datasets occur often in real-life classification problems. In credit card fraud detection, fraud may occur less than .01% of all transactions. The usual maximum likelihood or risk minimization methods put equal weight to each data point; thus, the estimation problem is overwhelmed by the majority class.&lt;/p&gt;

&lt;p&gt;Rare occurrences of an event are usually the most interesting part of the problem. We&amp;rsquo;d like a way to compensate for this asymmetry.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Adding weights / cost function&lt;/h3&gt;

&lt;p&gt;The easiest thing to do is just add weights to each class so their approximately equal. Say there are 99 samples of class 0 and 1 sample of class 1, then we multiply (1/99) to class 0.&lt;/p&gt;

&lt;p&gt;In scikit-learn there usually an option to equalize the classes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from sklearn.svm import SVC

clf = SVC()
clf_fit = clf.fit(x, y, class_weight = &#39;auto&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For random forests:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import balance_weights

clf = RandomForestClassifier()
clf_fit = clf.fit(x, y, sample_weight = balance_weights(y))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There can also be a cost function to get the optimal weight proportions and treat the weights as hyperparameters.&lt;/p&gt;

&lt;p&gt;Unfortunately, a highly imbalanced dataset with highly asymmetric weights will have a very &amp;ldquo;spiky&amp;rdquo; optimization surface. Weights may work if the rare class has enough variance in the data. But most likely the classification results will be inconsistent.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Resampling&lt;/h3&gt;

&lt;p&gt;Another approach is bootstrap resampling. Resampling ameliorates the &amp;ldquo;spiky&amp;rdquo; problem somewhat by oversampling the rare class to match the sample size of the majority class[es]. The resampling can be combined with undersampling of the majority class[es].&lt;/p&gt;

&lt;p&gt;If the rare class is not sufficiently varied enough, the optimization surface can get &amp;ldquo;blocky.&amp;rdquo;&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;SMOTE (Synthetic Minority Oversampling Technique)&lt;/h3&gt;

&lt;p&gt;To increase the variability of the rare class, you can take advantage of the majority class. The &lt;a href=&#34;https://www.jair.org/media/953/live-953-2037-jair.pdf&#34;&gt;SMOTE&lt;/a&gt; was developed for this purpose. Resampling takes into account the correlations of the features between data points.&lt;/p&gt;

&lt;p&gt;The SMOTE algo. is as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For each data point for the rare class

&lt;ul&gt;
&lt;li&gt;Grab K nearest neighbors from the entire dataset&lt;/li&gt;
&lt;li&gt;For however many times you would like to resample

&lt;ul&gt;
&lt;li&gt;Randomly pick a neighbor&lt;/li&gt;
&lt;li&gt;Calculate the distance between the neighbor and the original data point&lt;/li&gt;
&lt;li&gt;Randomly choose a point between the two points and use that as a resample&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So if being poor and irresponsiveness are features that generally predict credit card fraud, you would want to resample the data with similar features. There is some randomization / shrinkage to smooth out resamples.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Test&lt;/h3&gt;

&lt;p&gt;So I ran a test using a random forests classifier on made up data. I generated imbalanced binary data where the ratio is 99:1. I calculated the ROC curve of the results for 1) just using weights to adjust the imbalanced, and 2) using the SMOTE algorithm.&lt;/p&gt;

&lt;p&gt;Full code in a &lt;a href=&#34;https://gist.github.com/tokestermw/487971ee8b297f5749d1&#34;&gt;Gist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/ROC_two.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;The red line shows the ROC curve for the SMOTE algorithm and the blue line for the weights method. I plotted the thresholds of classification as well. In general, the SMOTE method pushed the ROC curve to the upper left (better, but not by much). The bigger change comes from the thresholds. If we assume a canonical threshold of 0.5, the improvement to recall is about 0.22. We don&amp;rsquo;t want to rely on a tiny threshold since they are inconsistent.&lt;/p&gt;

&lt;p&gt;There are of course trade-offs to this method. The more similar you make the resamples to the original dataset, the harder it is to predict new rare cases outside of he dataset.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;The code for the SMOTE algorithm.&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;
SMOTE: Synthetic Minority Over-sampling Technique
Chawla, Bowyer, Hall, Kegelmeyer
Journal of Artificial Intelligence Research 16 (2002) 321-357

https://www.jair.org/media/953/live-953-2037-jair.pdf
&amp;quot;&amp;quot;&amp;quot;

import warnings
import random

import numpy as np
from sklearn.neighbors import NearestNeighbors

def smote(T, N, K):
    &amp;quot;&amp;quot;&amp;quot;
    T ~ an array-like object representing the minority matrix
    N ~ the percent oversampling you want. e.g. 500 will give you 5 samples
    from the SMOTE algorithm (thus, has to be multiple of 100).
    K ~ K Nearest Neighbors
    &amp;quot;&amp;quot;&amp;quot;

    ## make sure T is an array with the proper dimensions
    T = np.asarray(T, dtype = np.float)
    nsamples = T.shape[0]
    nfeatures = T.shape[1]
    if nsamples &amp;lt; nfeatures:
        warnings.warn(&amp;quot;Make sure the features are in the columns.&amp;quot;)

    ## we want to oversample
    if N &amp;lt; 100:
        raise Exception(&amp;quot;N should be at least 100&amp;quot;)

    N = int(N) / 100

    nn = NearestNeighbors(K)

    nn.fit(T)

    synthetic = np.zeros([N * nsamples, nfeatures])

    for sample in xrange(nsamples):
        nn_minority = nn.kneighbors(T[sample], return_distance = False)[0]
        N_next = N

        newindex = 0
        while N_next != 0:
            k_chosen = random.randint(0, K - 1)
            while nn_minority[k_chosen] == sample: # don&#39;t pick itself
                k_chosen = random.randint(0, K - 1)                

            for feature in xrange(nfeatures):
                diff = T[nn_minority[k_chosen], feature] - T[sample, feature]
                gap = random.uniform(0, 1)
                synthetic[N*sample + newindex, feature] = T[sample, feature] + gap * diff

            newindex += 1
            N_next -= 1

    return synthetic
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayes Classifier on Epicurious Recipes using Julia</title>
      <link>http://localhost:1313/posts/julia-naive-bayes</link>
      <pubDate>Sat, 12 Jul 2014 07:00:36 UTC</pubDate>
      
      <guid>http://localhost:1313/posts/julia-naive-bayes</guid>
      <description>

&lt;p&gt;This is adapted from an &lt;a href=&#34;https://github.com/JuliaLang/IJulia.jl&#34;&gt;IJulia&lt;/a&gt; Notebook. It runs a
naive Bayes classifier on some &lt;a href=&#34;http://www.epicurious.com&#34;&gt;Epicurious&lt;/a&gt; recipe
data as an exercise to learn more &lt;a href=&#34;http://julialang.org&#34;&gt;Julia&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A better, more Julia-like implementation of Naive Bayes is
&lt;a href=&#34;https://github.com/johnmyleswhite/NaiveBayes.jl&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Convenience functions&lt;/h3&gt;

&lt;p&gt;Each line of the data contains the name of the cuisine, then the list of
ingredients. I create a &lt;code&gt;RaggedMatrix&lt;/code&gt; type (alias) to deal with string data of
indeterminate length. The &lt;code&gt;parse&lt;/code&gt; function will split the data into ingredients
and cuisines. I also have a &lt;code&gt;count_vector&lt;/code&gt; that counts the items in a single
dimension array (or a vector in R sense, or a list in Python sense).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typealias RaggedMatrix{T} Array{Array{T,1},1}

function parse(filename)
    file = readlines(open(filename))

    x = convert(RaggedMatrix{String},
            [apply(vcat, [rstrip(term) for term in split(line, &#39;\t&#39;)[2:end]])
             for line in file])
    y = convert(Array{String,1},
            [split(line, &#39;\t&#39;)[1] for line in file])

    return(x, y)
end

function count_vector(v::Array)
    v_counts = Dict()
    v_uniq = unique(v)
    for name in v_uniq
        counts = sum([i == name for i in v])
        push!(v_counts, name, counts)
    end
    v_counts
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Data&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://yongyeol.com/data/scirep-cuisines-detail.zip&#34;&gt;Data&lt;/a&gt; is from Yong-Yeol Ahn from a paper that was fairly well noticed (food
networks, not that kind). There are other recipes but I decided to use
Epicurious.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filename = &amp;quot;../data/scirep-cuisines-detail/epic_recipes.txt&amp;quot;

x, y = parse(filename)

# number of recipes
length(y)

13408

# count all the cuisines
y_counts = count_vector(y)
[i for i in y_counts]

26-element Array{Any,1}:
 (&amp;quot;Mexican&amp;quot;,622)
 (&amp;quot;Chinese&amp;quot;,226)
 (&amp;quot;Moroccan&amp;quot;,137)
 (&amp;quot;Cajun_Creole&amp;quot;,146)
 (&amp;quot;Mediterranean&amp;quot;,289)
 (&amp;quot;Central_SouthAmerican&amp;quot;,241)  
 (&amp;quot;Southwestern&amp;quot;,108)
 (&amp;quot;English_Scottish&amp;quot;,204)
 (&amp;quot;Thai&amp;quot;,164)
 (&amp;quot;Japanese&amp;quot;,136)
 (&amp;quot;Scandinavian&amp;quot;,92)
 (&amp;quot;African&amp;quot;,115)
 (&amp;quot;Greek&amp;quot;,225)
 (&amp;quot;German&amp;quot;,52)
 (&amp;quot;Indian&amp;quot;,274)
 (&amp;quot;MiddleEastern&amp;quot;,248)
 (&amp;quot;American&amp;quot;,4988)
 (&amp;quot;Asian&amp;quot;,1176)
 (&amp;quot;French&amp;quot;,996)
 (&amp;quot;Irish&amp;quot;,86)
 (&amp;quot;Italian&amp;quot;,1715)
 (&amp;quot;Jewish&amp;quot;,320)
 (&amp;quot;Spanish_Portuguese&amp;quot;,291)
 (&amp;quot;Vietnamese&amp;quot;,65)
 (&amp;quot;EasternEuropean_Russian&amp;quot;,146)
 (&amp;quot;Southern_SoulFood&amp;quot;,346)

# each document is a recipe
y[1], x[1]

(&amp;quot;Vietnamese&amp;quot;,String[&amp;quot;vinegar&amp;quot;,&amp;quot;cilantro&amp;quot;,&amp;quot;mint&amp;quot;,&amp;quot;olive_oil&amp;quot;,&amp;quot;cayenne&amp;quot;,&amp;quot;fish&amp;quot;,&amp;quot;lime_juice&amp;quot;,&amp;quot;shrimp&amp;quot;,&amp;quot;lettuce&amp;quot;,&amp;quot;carrot&amp;quot;,&amp;quot;garlic&amp;quot;,&amp;quot;basil&amp;quot;,&amp;quot;cucumber&amp;quot;,&amp;quot;rice&amp;quot;,&amp;quot;seed&amp;quot;,&amp;quot;shiitake&amp;quot;])

# create lookup dictionaries
all_cuisines = unique(y)
cuisines = (String=&amp;gt;Int64)[all_cuisines[i] =&amp;gt; i for i in 1:length(all_cuisines)]
rev_cuisines = (Int64=&amp;gt;String)[cuisines[i] =&amp;gt; i for i in all_cuisines]

all_ingredients = unique(apply(vcat, [line for line in x]))
ingredients = (String=&amp;gt;Int64)[all_ingredients[i] =&amp;gt; i for i in 1:length(all_ingredients)]
rev_ingredients = (Int64=&amp;gt;String)[ingredients[i] =&amp;gt; i for i in all_ingredients]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Vectorize -&amp;gt; Fit -&amp;gt; Predict&lt;/h3&gt;

&lt;p&gt;Vaguely following the sci-kit model, I write functions for vectorization, fitting
and prediction.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ingredients x cuisines (feature x class) matrix
# count the words for each cuisine
function count_vectorizer(x::RaggedMatrix{String}, y::Array{String,1},
    features::Dict{String,Int64}, classes::Dict{String,Int64})


    X = zeros(length(features), length(classes))

    # Julia is column dominant
    for c in keys(classes)
        features_for_class = apply(vcat, [i for i in x[find(_ -&amp;gt; _ == c, y)]])
        for f in features_for_class
            @inbounds X[features[f], classes[c]] += 1.0
        end
    end
    X
end

@timed X = count_vectorizer(x, y, ingredients, cuisines)

(
350x26 Array{Float64,2}:
 15.0   31.0   79.0  18.0  127.0   41.0  …   11.0  56.0  20.0  397.0  17.0
 24.0   79.0   30.0  18.0    7.0   75.0      43.0   5.0   4.0  244.0  56.0
 21.0   35.0    7.0  14.0   19.0    7.0      20.0   2.0   4.0   97.0   2.0
  2.0   40.0  183.0  80.0  307.0   75.0     100.0  47.0  27.0   78.0  43.0
 30.0  125.0   70.0  21.0   38.0  125.0      40.0  84.0   3.0  345.0  88.0
 51.0   11.0   24.0  14.0   68.0    8.0  …   10.0   2.0   6.0  228.0   0.0
 30.0   26.0   15.0   3.0    8.0   41.0       3.0   4.0   3.0  195.0  29.0
 12.0   18.0   31.0   0.0   12.0   13.0       1.0  14.0   1.0  142.0   2.0
  9.0    5.0    3.0   3.0   21.0    8.0       0.0   3.0   6.0   58.0   6.0
 18.0   21.0   12.0  40.0  108.0    7.0      21.0  11.0   8.0  145.0   6.0
 47.0  114.0  167.0  60.0  272.0  137.0  …   63.0  89.0  19.0  565.0  67.0
 18.0    3.0    6.0   4.0   69.0    1.0       6.0   8.0   5.0   66.0   0.0
 11.0   20.0   15.0   8.0   23.0    3.0       4.0   2.0   8.0   92.0   2.0
  ⋮                                 ⋮    ⋱                             ⋮  
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0  …    0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0  …    0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   1.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   1.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    1.0   0.0,

0.44882671,10309732)

print(sum(X, 1))
print(size(X))

[737.0 2804.0 2664.0 2510.0 8279.0 2184.0 1715.0 1957.0 759.0 1997.0 43541.0 1220.0 2330.0 1311.0 14436.0 685.0 5695.0 2177.0 504.0 2498.0 1160.0 1482.0 3055.0 1658.0 11644.0 1123.0](350,26)

function naive_bayes_fit(X::Array{Float64,2}, alpha::Float64 = 1.0)

    log_like = zeros(size(X))

    # sum over classes
    total = sum(X, 1)

    # sum over rows for each class for performance reasons
    for j in 1:size(X)[2]
        @inbounds log_like[:,j] = log(X[:,j] .+ alpha) .- log(total[j])
    end

    log_like
end

@timed log_like = naive_bayes_fit(X)

(
350x26 Array{Float64,2}:
 -3.83     -4.47307  -3.50556  -4.8836   …  -4.36884  -3.37609  -4.13339
 -3.38371  -3.55678  -4.4536   -4.8836      -5.80393  -3.86129  -2.98071
 -3.51155  -4.35528  -5.80814  -5.11999     -5.80393  -4.77758  -5.92515
 -5.50398  -4.22523  -2.67265  -3.43359     -4.08116  -4.9931   -3.23957
 -3.1686   -3.10252  -3.6249   -4.737       -6.02707  -3.51611  -2.53512
 -2.65134  -5.4539   -4.66871  -5.11999  …  -5.46746  -3.92882  -7.02376
 -3.1686   -4.64297  -5.115    -6.44174     -6.02707  -4.08443  -3.62256
 -4.03764  -4.99436  -4.42185  -7.82804     -6.72022  -4.3997   -5.92515
 -4.3      -6.14704  -6.50129  -6.44174     -5.46746  -5.28501  -5.07785
 -3.65815  -4.84776  -5.32263  -4.11447     -5.21614  -4.37894  -5.07785
 -2.73139  -3.19387  -2.76362  -3.71716  …  -4.41764  -3.02395  -2.80425
 -3.65815  -6.55251  -5.94167  -6.2186      -5.62161  -5.15785  -7.02376
 -4.11768  -4.89428  -5.115    -5.63081     -5.21614  -4.82995  -5.92515
  ⋮                                      ⋱                       ⋮
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804  …  -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804  …  -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -6.72022  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -8.6694   -7.02376,

0.020244006,644824)

function naive_bayes_predict(predict::Array{String,1}, log_like::Array{Float64,2},
    prior::Array{Float64,1}, features::Dict{String,Int64}, rev_classes::Dict{Int64,String})

    results = zeros(length(rev_classes))

    for i in 1:length(rev_classes)
        ind = convert(Array{Int64,1}, [features[i] for i in predict]) # not sure why i have to do this
        results[i] = sum(log_like[ind, i]) + log(prior[i])
    end

    rev_classes[indmax(results)]
end

prior = ones(length(cuisines)) / length(cuisines)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Prediction&lt;/h3&gt;

&lt;p&gt;Now I can predict the cuisine given a list of ingredients.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# A prediction
i = 3
y[i], naive_bayes_predict(x[i], log_like, prior, ingredients, rev_cuisines), x[i]

(&amp;quot;Vietnamese&amp;quot;,&amp;quot;Vietnamese&amp;quot;,String[&amp;quot;garlic&amp;quot;,&amp;quot;soy_sauce&amp;quot;,&amp;quot;lime_juice&amp;quot;,&amp;quot;thai_pepper&amp;quot;])

# another one
i = 3000
y[i], naive_bayes_predict(x[i], log_like, prior, ingredients, rev_cuisines), x[i]

(&amp;quot;American&amp;quot;,&amp;quot;German&amp;quot;,String[&amp;quot;cane_molasses&amp;quot;,&amp;quot;butter&amp;quot;,&amp;quot;wheat&amp;quot;,&amp;quot;vanilla&amp;quot;,&amp;quot;ginger&amp;quot;,&amp;quot;honey&amp;quot;,&amp;quot;nutmeg&amp;quot;,&amp;quot;cinnamon&amp;quot;,&amp;quot;lard&amp;quot;,&amp;quot;egg&amp;quot;,&amp;quot;milk&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I can create a confusion matrix to see the overlap of the ingredients between
cuisines.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confusion_matrix = zeros(length(cuisines), length(cuisines))

for i in keys(cuisines)
    cuisine = x[find(_ -&amp;gt; _ == i, y)]
    total = length(cuisine)
    for j in cuisine
        predicted = naive_bayes_predict(j, log_like, prior, ingredients, rev_cuisines)
        confusion_matrix[cuisines[i], cuisines[predicted]] += 1 / total
    end
end

confusion_matrix

26x26 Array{Float64,2}:
 0.907692    0.0         0.0         …  0.0         0.0         0.0
 0.0218978   0.627737    0.0            0.0328467   0.00364964  0.0255474
 0.0274914   0.00343643  0.42268        0.0824742   0.0         0.0652921
 0.00625     0.009375    0.0125         0.065625    0.0         0.021875  
 0.0100402   0.00401606  0.0291165      0.195783    0.00100402  0.0060241
 0.0207469   0.0165975   0.0746888   …  0.0829876   0.00414938  0.190871  
 0.0205479   0.00684932  0.0            0.0616438   0.00684932  0.0273973
 0.365854    0.0304878   0.0            0.0         0.0304878   0.00609756
 0.0         0.0108696   0.0            0.0869565   0.0         0.0
 0.0         0.00444444  0.00444444     0.0311111   0.0         0.00444444
 0.0280674   0.0114274   0.0252606   …  0.152165    0.00240577  0.0643545
 0.026087    0.0521739   0.0173913      0.0347826   0.0         0.00869565
 0.00806452  0.0322581   0.0201613      0.0322581   0.0         0.016129  
 0.00684932  0.00684932  0.0205479      0.143836    0.0         0.0273973
 0.00116618  0.00116618  0.0268222      0.0874636   0.0         0.0110787
 0.0         0.0         0.0         …  0.0930233   0.0         0.0
 0.0369775   0.00803859  0.0144695      0.0498392   0.0         0.302251  
 0.039823    0.00884956  0.0            0.00884956  0.0265487   0.00442478
 0.0         0.0         0.0            0.115385    0.0         0.0192308
 0.0         0.0103806   0.0484429      0.0484429   0.0         0.0138408
 0.0441176   0.00735294  0.0         …  0.0220588   0.0147059   0.0
 0.00729927  0.0364964   0.0145985      0.0145985   0.0         0.00729927
 0.00867052  0.00289017  0.0144509      0.106936    0.0         0.0606936
 0.00980392  0.00980392  0.0196078      0.485294    0.0         0.00490196
 0.182823    0.0280612   0.0            0.0102041   0.0637755   0.00935374
 0.00925926  0.00925926  0.0185185   …  0.00925926  0.0185185   0.685185  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What is the cuisine that is easiest to predict? Vietnamese.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confuse = diag(confusion_matrix)
sort([(rev_cuisines[i], confuse[i]) for i in 1:length(cuisines)])

26-element Array{(Any,Any),1}:
 (&amp;quot;African&amp;quot;,0.32173913043478275)
 (&amp;quot;American&amp;quot;,0.00541299117882919)
 (&amp;quot;Asian&amp;quot;,0.06377551020408168)
 (&amp;quot;Cajun_Creole&amp;quot;,0.7054794520547963)
 (&amp;quot;Central_SouthAmerican&amp;quot;,0.2531120331950211)
 (&amp;quot;Chinese&amp;quot;,0.756637168141594)
 (&amp;quot;EasternEuropean_Russian&amp;quot;,0.26027397260273977)
 (&amp;quot;English_Scottish&amp;quot;,0.48529411764705815)
 (&amp;quot;French&amp;quot;,0.14156626506024103)
 (&amp;quot;German&amp;quot;,0.7692307692307696)
 (&amp;quot;Greek&amp;quot;,0.6088888888888881)
 (&amp;quot;Indian&amp;quot;,0.6277372262773735)
 (&amp;quot;Irish&amp;quot;,0.6627906976744192)
 (&amp;quot;Italian&amp;quot;,0.4740524781341024)
 (&amp;quot;Japanese&amp;quot;,0.7794117647058805)
 (&amp;quot;Jewish&amp;quot;,0.3406249999999993)
 (&amp;quot;Mediterranean&amp;quot;,0.33217993079584757)
 (&amp;quot;Mexican&amp;quot;,0.29099678456591677)
 (&amp;quot;MiddleEastern&amp;quot;,0.3225806451612901)
 (&amp;quot;Moroccan&amp;quot;,0.6131386861313863)
 (&amp;quot;Scandinavian&amp;quot;,0.6304347826086959)
 (&amp;quot;Southern_SoulFood&amp;quot;,0.3930635838150301)
 (&amp;quot;Southwestern&amp;quot;,0.6851851851851855)
 (&amp;quot;Spanish_Portuguese&amp;quot;,0.422680412371133)
 (&amp;quot;Thai&amp;quot;,0.4878048780487808)
 (&amp;quot;Vietnamese&amp;quot;,0.9076923076923062)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like &amp;ldquo;Asian&amp;rdquo; cuisine is so general that the classifier cannot predict
correctly. Let&amp;rsquo;s see what it&amp;rsquo;s confused with.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confuse = confusion_matrix[cuisines[&amp;quot;Asian&amp;quot;],:]
[(rev_cuisines[i], confuse[i]) for i in 1:length(cuisines)]

26-element Array{(Any,Any),1}:
 (&amp;quot;Vietnamese&amp;quot;,0.18282312925170086)
 (&amp;quot;Indian&amp;quot;,0.028061224489795925)
 (&amp;quot;Spanish_Portuguese&amp;quot;,0.0)
 (&amp;quot;Jewish&amp;quot;,0.004251700680272108)
 (&amp;quot;French&amp;quot;,0.002551020408163265)
 (&amp;quot;Central_SouthAmerican&amp;quot;,0.0008503401360544217)  
 (&amp;quot;Cajun_Creole&amp;quot;,0.005952380952380952)
 (&amp;quot;Thai&amp;quot;,0.10034013605442185)
 (&amp;quot;Scandinavian&amp;quot;,0.006802721088435374)
 (&amp;quot;Greek&amp;quot;,0.002551020408163265)
 (&amp;quot;American&amp;quot;,0.0008503401360544217)
 (&amp;quot;African&amp;quot;,0.006802721088435374)
 (&amp;quot;MiddleEastern&amp;quot;,0.0017006802721088435)
 (&amp;quot;EasternEuropean_Russian&amp;quot;,0.0008503401360544217)
 (&amp;quot;Italian&amp;quot;,0.002551020408163265)
 (&amp;quot;Irish&amp;quot;,0.003401360544217687)
 (&amp;quot;Mexican&amp;quot;,0.002551020408163265)
 (&amp;quot;Chinese&amp;quot;,0.31037414965986226)
 (&amp;quot;German&amp;quot;,0.028911564625850348)
 (&amp;quot;Mediterranean&amp;quot;,0.0)
 (&amp;quot;Japanese&amp;quot;,0.2176870748299322)
 (&amp;quot;Moroccan&amp;quot;,0.00510204081632653)
 (&amp;quot;Southern_SoulFood&amp;quot;,0.0017006802721088435)
 (&amp;quot;English_Scottish&amp;quot;,0.010204081632653059)
 (&amp;quot;Asian&amp;quot;,0.06377551020408168)
 (&amp;quot;Southwestern&amp;quot;,0.009353741496598638)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yup, &amp;ldquo;Asian&amp;rdquo; is confused with other Asian cuisines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>D3 and jstat to visualize statistics</title>
      <link>http://localhost:1313/posts/d3-with-jstat</link>
      <pubDate>Sat, 21 Sep 2013 00:00:00 UTC</pubDate>
      
      <guid>http://localhost:1313/posts/d3-with-jstat</guid>
      <description>&lt;p&gt;R does plotting statistics well, D3.js does plotting web-friendly interactive plots well. So I decided to weave &lt;a href=&#34;http://www.jstat.org/&#34;&gt;jstat&lt;/a&gt; (a Javascript stats library) into D3 to plot R-like objects. One of the things people during data analysis is to plot theoretical distributions to check it against the data. In jstat, you can easily do this by using the normal pdf function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var range = jStat(jStat.seq(-5,5,100));
var densities = range.normal(0.0,1.0).pdf()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can feed this to a &amp;ldquo;data frame&amp;rdquo; by using normal Javascript.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var dataset = [], zerodata = [];
for (var i = 0; i &amp;lt; range[0].length; i++) {
    dataset.push([range[0][i], densities[0][i]]);
    zerodata.push([0, 0]);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The best feature in D3 is its interpolating algorithm to go from one shape to another. You can create a &lt;code&gt;d3.svg.area&lt;/code&gt; object to define an area from the bottom y = 0, to the top of the distribution.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:::javascript
var pdfArea = d3.svg.area()
    .x(function(d) { return xscale(d[0]) })
    .y1(function(d) { return yscale(d[1]) })
    .y0(h)
    .interpolate(&amp;quot;basis&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the populated data frame from above, you can append a &lt;code&gt;path&lt;/code&gt; going from &lt;code&gt;zerodata&lt;/code&gt; (at y = 0) to &lt;code&gt;dataset&lt;/code&gt; (at y = dnorm(x)).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;svg.append(&amp;quot;path&amp;quot;)
    .attr(&amp;quot;d&amp;quot;, pdfArea(zerodata))
    .attr(&amp;quot;fill&amp;quot;, &amp;quot;green&amp;quot;)
    .transition().delay(500).duration(1000)
    .attr(&amp;quot;d&amp;quot;, pdfArea(dataset))
    .attr(&amp;quot;fill&amp;quot;, d3.rgb(255, 124, 138));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add some more features like clicking.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;svg.on(&amp;quot;click&amp;quot;, function() {
    newmean = Math.random() * 6 - 3;
    newden = range.normal(newmean,1.0).pdf()
    var newdata = [];
    for (var i = 0; i &amp;lt; range[0].length; i++) {
        newdata.push([range[0][i], newden[0][i]]);
    }
    svg.select(&amp;quot;path&amp;quot;)
        .attr(&amp;quot;d&amp;quot;, pdfArea(zerodata))
        .attr(&amp;quot;fill&amp;quot;, &amp;quot;pink&amp;quot;)
        .transition().delay(500).duration(1000)
        .attr(&amp;quot;d&amp;quot;, pdfArea(newdata))
        .attr(&amp;quot;fill&amp;quot;, d3.rgb(255, 124, 138));
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final result should be in the sidebar.&lt;/p&gt;

&lt;p&gt;Entire code is posted in &lt;a href=&#34;http://bl.ocks.org/tokestermw/6652994&#34;&gt;bl.ocks&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>