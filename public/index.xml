<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Recent Content on Motoki Wu </title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://tokestermw.github.io/index.xml</link>
    <language>en-us</language>
    
    
    <updated>Sun, 26 Oct 2014 00:00:00 UTC</updated>
    
    <item>
      <title>Visualization of Topic Models</title>
      <link>http://tokestermw.github.io/posts/topicmodel-viz</link>
      <pubDate>Sun, 26 Oct 2014 00:00:00 UTC</pubDate>
      
      <guid>http://tokestermw.github.io/posts/topicmodel-viz</guid>
      <description>

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;Lies, damned lies and topic modeling.&lt;/p&gt;&amp;mdash; Motoki Wu (@plusepsilon) &lt;a href=&#34;https://twitter.com/plusepsilon/status/526464306754240513&#34;&gt;October 26, 2014&lt;/a&gt;&lt;/blockquote&gt; &lt;/p&gt;

&lt;p&gt;There is still much work to do in interpreting and visualizing topic model word clusters so some tangible results can be derived in a research setting or something useful comes out in a production app. A cluster of words are likely to be noisy and illegible so I will be exploring a few options presented &lt;a href=&#34;http://tedunderwood.com/2012/11/11/visualizing-topic-models/&#34;&gt;here&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/Ted_Underwood&#34;&gt;@Ted_Underwood&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I took some Twitter data from &lt;a href=&#34;http://oaklandwiki.org/2013_BART_Strikes?&amp;amp;redirected_from=july%201st%202013%20strike&#34;&gt;last year&amp;rsquo;s BART strike&lt;/a&gt; used in a previous &lt;a href=&#34;http://nbviewer.ipython.org/github/tokestermw/twitter-bart/blob/master/ipynb/Twitter140-checkNB.ipynb&#34;&gt;IPython notebook&lt;/a&gt;. This data contains the following chain of events:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;strike announced that stops all trains from going from Bay Area suburbs to San Francisco&lt;/li&gt;
&lt;li&gt;a week of stand off when two BART workers were killed in an accident&lt;/li&gt;
&lt;li&gt;end of strike&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The hope is that topic modeling can give indication of the above three discrete events. It would be even better if it can identify pro- and anti-strike tweeets as well.&lt;/p&gt;

&lt;p&gt;The topic model is a standard &lt;a href=&#34;http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&#34;&gt;Latent Dirichlet Allocation&lt;/a&gt; fitted using &lt;a href=&#34;http://radimrehurek.com/gensim/&#34;&gt;gensim&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://gist.github.com/tokestermw/3588e6fbbb2f03f89798&#34;&gt;Full code&lt;/a&gt; in a Gist.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;List of Words&lt;/h3&gt;

&lt;p&gt;Not really a visualization, but the easiest thing to do is to grab the top words of each topic and list them. Here is a sample:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Top 10 terms for topic #0: expected, rep, announces, over, looks, ktvu, want, like, guys, will
Top 10 terms for topic #1: sfgate, gridlock, normal, happy, head, real, ferries, over, runs, exactly
Top 10 terms for topic #2: a.m., another, vote, members, other, reason, let, pass, serâ€¦, major
Top 10 terms for topic #3: today, killed, sanfranmag, hopes, ave, expect, delays, running, home, train
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Topic 3 is most likely grouping tweets about the BART workers that were killed and to expect delays because of the incident. Topic 2 looks like union members throwing a strike are voting on .. something. Topic 1 is talking about ferries (the only other way of getting to San Francisco aside from driving) but it is not clear what these tweets are happy about. Topic 0 is looks to be random.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Word clouds&lt;/h3&gt;

&lt;p&gt;Word clouds is a simple alternative to looking at the same data. By embiggening the most distinct words, it is easier find import words with a quick scan.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/terms1.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Quan is a mayor of Oakland, and this topic seems to say that the deal is about to be reached.&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;PCA&lt;/h3&gt;

&lt;p&gt;It gets progressively more difficult to distinguish chaff from the signal as the number of topics increase. Reducing the topics is sometimes not an option because the topics get too general. It also becomes more difficult to automate the results so a person that haven&amp;rsquo;t been looking at topic clusers for hours on end can understand.&lt;/p&gt;

&lt;p&gt;PCA is a canonical way of reducing dimensions and it is a method that can of course be used here as well. Taking the probabities of each word for each topic, PCA can collapse the data into 2 dimensions. This makes it easily plottable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/pca_topic.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Interesting how topci 19, 21 and to a some extent, 31 deviates from the other topics (I used 40 topics). Looking athe innards of each topic, all three seems to be talking about the end of the strike.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [231]: lda.show_topic(19)
Out[231]:
[(0.18418766385173357, u&#39;tuesday&#39;),
(0.10941284772798156, u&#39;over&#39;),
(0.074073551230934093, u&#39;deal&#39;),
(0.057823820985690839, u&#39;reached&#39;),
(0.040004840107066328, u&#39;start&#39;),
(0.014618710538754369, u&#39;chrisfilippi&#39;),
(0.01175792963040383, u&#39;commute&#39;),
(0.010096535268990677, u&#39;buses&#39;),
(0.0099316408990382157, u&#39;abc7newsbayarea&#39;),
(0.0089298280179637094, u&#39;late&#39;)]

In [232]: lda.show_topic(21)
Out[232]:
[(0.19463842681026511, u&#39;over&#39;),
(0.039005911200223162, u&#39;rosenbergmerc&#39;),
(0.034922463036658115, u&#39;all&#39;),
(0.029778810358060626, u&#39;thank&#39;),
(0.018876961986207651, u&#39;unions&#39;),
(0.018656417067857364, u&#39;hopefully&#39;),
(0.016893084271683283, u&#39;pissed&#39;),
(0.013589706807636848, u&#39;someone&#39;),
(0.012154548491692339, u&#39;per&#39;),
(0.011787301022370321, u&#39;kron4news&#39;)]

In [233]: lda.show_topic(31)
Out[233]:
[(0.073542501022656165, u&#39;tentative&#39;),
(0.068444064522636891, u&#39;reach&#39;),
(0.057170204108103105, u&#39;run&#39;),
(0.054732038737147118, u&#39;trains&#39;),
(0.054298622740944123, u&#39;contract&#39;),
(0.046550628739041838, u&#39;unions&#39;),
(0.041191568872948219, u&#39;deal&#39;),
(0.040003874892020903, u&#39;abc7newsbayarea&#39;),
(0.030594570247699304, u&#39;agreement&#39;),
(0.025459332467351173, u&#39;announcement&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By flipping the matrix, it is trivial to repeat the analysis for words.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/pca_words.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;That is encouraging as most distinct words seem to be coming from the end of the strike. On the other hand, it is hard to tell if there are any further interesting subgroups.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Dendrogram&lt;/h3&gt;

&lt;p&gt;So far I&amp;rsquo;ve used the probability weights of each word in each topic. This topic x word matrix can be transformed into a topic by topic distance (correlation) matrix to do some additional data transformations. One is clustering. Hierarchical clustering is one such technique and easily plottable as a dendrogram.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/corr.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;The results look similar to PCA except it is easier to see the hierarchical ordering of the topics. PCA on the other hand can use any number of dimensions to see in which direction the vectors are different.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Force-directed graph&lt;/h3&gt;

&lt;p&gt;Using the same distance matrix, a force-directed graph can also be plotted. This basically plots the points spaced out in a certain way such that the farther the distance between two topics, the weaker the edge. I found this method to be sensitive to assumptions and it doesn&amp;rsquo;t help that I don&amp;rsquo;t really understand how the spacing works.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/network.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;&amp;hellip;&lt;/h3&gt;

&lt;p&gt;So that&amp;rsquo;s it. Four different ways to visualize topic modeling clusters. They may not be so useless after all, just that the uncertainty needs to be tamed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SMOTE Algorithm to Classify Imbalanced Datasets with Random Forests</title>
      <link>http://tokestermw.github.io/posts/imbalanced-datasets-random-forests</link>
      <pubDate>Tue, 29 Jul 2014 00:00:00 UTC</pubDate>
      
      <guid>http://tokestermw.github.io/posts/imbalanced-datasets-random-forests</guid>
      <description>

&lt;p&gt;Imbalanced datasets occur often in real-life classification problems. In credit card fraud detection, fraud may occur less than .01% of all transactions. The usual maximum likelihood or risk minimization methods put equal weight to each data point; thus, the estimation problem is overwhelmed by the majority class.&lt;/p&gt;

&lt;p&gt;Rare occurrences of an event are usually the most interesting part of the problem. We&amp;rsquo;d like a way to compensate for this asymmetry.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Adding weights / cost function&lt;/h3&gt;

&lt;p&gt;The easiest thing to do is just add weights to each class so their approximately equal. Say there are 99 samples of class 0 and 1 sample of class 1, then we multiply (1/99) to class 0.&lt;/p&gt;

&lt;p&gt;In scikit-learn there usually an option to equalize the classes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from sklearn.svm import SVC

clf = SVC()
clf_fit = clf.fit(x, y, class_weight = &#39;auto&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For random forests:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import balance_weights

clf = RandomForestClassifier()
clf_fit = clf.fit(x, y, sample_weight = balance_weights(y))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There can also be a cost function to get the optimal weight proportions and treat the weights as hyperparameters.&lt;/p&gt;

&lt;p&gt;Unfortunately, a highly imbalanced dataset with highly asymmetric weights will have a very &amp;ldquo;spiky&amp;rdquo; optimization surface. Weights may work if the rare class has enough variance in the data. But most likely the classification results will be inconsistent.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Resampling&lt;/h3&gt;

&lt;p&gt;Another approach is bootstrap resampling. Resampling ameliorates the &amp;ldquo;spiky&amp;rdquo; problem somewhat by oversampling the rare class to match the sample size of the majority class[es]. The resampling can be combined with undersampling of the majority class[es].&lt;/p&gt;

&lt;p&gt;If the rare class is not sufficiently varied enough, the optimization surface can get &amp;ldquo;blocky.&amp;rdquo;&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;SMOTE (Synthetic Minority Oversampling Technique)&lt;/h3&gt;

&lt;p&gt;To increase the variability of the rare class, you can take advantage of the majority class. The &lt;a href=&#34;https://www.jair.org/media/953/live-953-2037-jair.pdf&#34;&gt;SMOTE&lt;/a&gt; was developed for this purpose. Resampling takes into account the correlations of the features between data points.&lt;/p&gt;

&lt;p&gt;The SMOTE algo. is as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For each data point for the rare class

&lt;ul&gt;
&lt;li&gt;Grab K nearest neighbors from the entire dataset&lt;/li&gt;
&lt;li&gt;For however many times you would like to resample

&lt;ul&gt;
&lt;li&gt;Randomly pick a neighbor&lt;/li&gt;
&lt;li&gt;Calculate the distance between the neighbor and the original data point&lt;/li&gt;
&lt;li&gt;Randomly choose a point between the two points and use that as a resample&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So if being poor and irresponsiveness are features that generally predict credit card fraud, you would want to resample the data with similar features. There is some randomization / shrinkage to smooth out resamples.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Test&lt;/h3&gt;

&lt;p&gt;So I ran a test using a random forests classifier on made up data. I generated imbalanced binary data where the ratio is 99:1. I calculated the ROC curve of the results for 1) just using weights to adjust the imbalanced, and 2) using the SMOTE algorithm.&lt;/p&gt;

&lt;!-- Full code in a [Gist](https://gist.github.com/tokestermw/487971ee8b297f5749d1). --&gt;

&lt;p&gt;&lt;img src=&#34;../../images/ROC_two.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;The red line shows the ROC curve for the SMOTE algorithm and the blue line for the weights method. I plotted the thresholds of classification as well. In general, the SMOTE method pushed the ROC curve to the upper left (better, but not by much). The bigger change comes from the thresholds. If we assume a canonical threshold of 0.5, the improvement to recall is about 0.22. We don&amp;rsquo;t want to rely on a tiny threshold since they are inconsistent.&lt;/p&gt;

&lt;p&gt;There are of course trade-offs to this method. The more similar you make the resamples to the original dataset, the harder it is to predict new rare cases outside of he dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayes Classifier on Epicurious Recipes using Julia</title>
      <link>http://tokestermw.github.io/posts/julia-naive-bayes</link>
      <pubDate>Sat, 12 Jul 2014 07:00:36 UTC</pubDate>
      
      <guid>http://tokestermw.github.io/posts/julia-naive-bayes</guid>
      <description>

&lt;p&gt;This is adapted from an &lt;a href=&#34;https://github.com/JuliaLang/IJulia.jl&#34;&gt;IJulia&lt;/a&gt; Notebook. It runs a
naive Bayes classifier on some &lt;a href=&#34;http://www.epicurious.com&#34;&gt;Epicurious&lt;/a&gt; recipe
data as an exercise to learn more &lt;a href=&#34;http://julialang.org&#34;&gt;Julia&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A better, more Julia-like implementation of Naive Bayes is
&lt;a href=&#34;https://github.com/johnmyleswhite/NaiveBayes.jl&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Convenience functions&lt;/h3&gt;

&lt;p&gt;Each line of the data contains the name of the cuisine, then the list of
ingredients. I create a &lt;code&gt;RaggedMatrix&lt;/code&gt; type (alias) to deal with string data of
indeterminate length. The &lt;code&gt;parse&lt;/code&gt; function will split the data into ingredients
and cuisines. I also have a &lt;code&gt;count_vector&lt;/code&gt; that counts the items in a single
dimension array (or a vector in R sense, or a list in Python sense).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typealias RaggedMatrix{T} Array{Array{T,1},1}

function parse(filename)
    file = readlines(open(filename))

    x = convert(RaggedMatrix{String},
            [apply(vcat, [rstrip(term) for term in split(line, &#39;\t&#39;)[2:end]])
             for line in file])
    y = convert(Array{String,1},
            [split(line, &#39;\t&#39;)[1] for line in file])

    return(x, y)
end

function count_vector(v::Array)
    v_counts = Dict()
    v_uniq = unique(v)
    for name in v_uniq
        counts = sum([i == name for i in v])
        push!(v_counts, name, counts)
    end
    v_counts
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Data&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://yongyeol.com/data/scirep-cuisines-detail.zip&#34;&gt;Data&lt;/a&gt; is from Yong-Yeol Ahn from a paper that was fairly well noticed (food
networks, not that kind). There are other recipes but I decided to use
Epicurious.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filename = &amp;quot;../data/scirep-cuisines-detail/epic_recipes.txt&amp;quot;

x, y = parse(filename)

# number of recipes
length(y)

13408

# count all the cuisines
y_counts = count_vector(y)
[i for i in y_counts]

26-element Array{Any,1}:
 (&amp;quot;Mexican&amp;quot;,622)
 (&amp;quot;Chinese&amp;quot;,226)
 (&amp;quot;Moroccan&amp;quot;,137)
 (&amp;quot;Cajun_Creole&amp;quot;,146)
 (&amp;quot;Mediterranean&amp;quot;,289)
 (&amp;quot;Central_SouthAmerican&amp;quot;,241)  
 (&amp;quot;Southwestern&amp;quot;,108)
 (&amp;quot;English_Scottish&amp;quot;,204)
 (&amp;quot;Thai&amp;quot;,164)
 (&amp;quot;Japanese&amp;quot;,136)
 (&amp;quot;Scandinavian&amp;quot;,92)
 (&amp;quot;African&amp;quot;,115)
 (&amp;quot;Greek&amp;quot;,225)
 (&amp;quot;German&amp;quot;,52)
 (&amp;quot;Indian&amp;quot;,274)
 (&amp;quot;MiddleEastern&amp;quot;,248)
 (&amp;quot;American&amp;quot;,4988)
 (&amp;quot;Asian&amp;quot;,1176)
 (&amp;quot;French&amp;quot;,996)
 (&amp;quot;Irish&amp;quot;,86)
 (&amp;quot;Italian&amp;quot;,1715)
 (&amp;quot;Jewish&amp;quot;,320)
 (&amp;quot;Spanish_Portuguese&amp;quot;,291)
 (&amp;quot;Vietnamese&amp;quot;,65)
 (&amp;quot;EasternEuropean_Russian&amp;quot;,146)
 (&amp;quot;Southern_SoulFood&amp;quot;,346)

# each document is a recipe
y[1], x[1]

(&amp;quot;Vietnamese&amp;quot;,String[&amp;quot;vinegar&amp;quot;,&amp;quot;cilantro&amp;quot;,&amp;quot;mint&amp;quot;,&amp;quot;olive_oil&amp;quot;,&amp;quot;cayenne&amp;quot;,&amp;quot;fish&amp;quot;,&amp;quot;lime_juice&amp;quot;,&amp;quot;shrimp&amp;quot;,&amp;quot;lettuce&amp;quot;,&amp;quot;carrot&amp;quot;,&amp;quot;garlic&amp;quot;,&amp;quot;basil&amp;quot;,&amp;quot;cucumber&amp;quot;,&amp;quot;rice&amp;quot;,&amp;quot;seed&amp;quot;,&amp;quot;shiitake&amp;quot;])

# create lookup dictionaries
all_cuisines = unique(y)
cuisines = (String=&amp;gt;Int64)[all_cuisines[i] =&amp;gt; i for i in 1:length(all_cuisines)]
rev_cuisines = (Int64=&amp;gt;String)[cuisines[i] =&amp;gt; i for i in all_cuisines]

all_ingredients = unique(apply(vcat, [line for line in x]))
ingredients = (String=&amp;gt;Int64)[all_ingredients[i] =&amp;gt; i for i in 1:length(all_ingredients)]
rev_ingredients = (Int64=&amp;gt;String)[ingredients[i] =&amp;gt; i for i in all_ingredients]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Vectorize -&amp;gt; Fit -&amp;gt; Predict&lt;/h3&gt;

&lt;p&gt;Vaguely following the sci-kit model, I write functions for vectorization, fitting
and prediction.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ingredients x cuisines (feature x class) matrix
# count the words for each cuisine
function count_vectorizer(x::RaggedMatrix{String}, y::Array{String,1},
    features::Dict{String,Int64}, classes::Dict{String,Int64})


    X = zeros(length(features), length(classes))

    # Julia is column dominant
    for c in keys(classes)
        features_for_class = apply(vcat, [i for i in x[find(_ -&amp;gt; _ == c, y)]])
        for f in features_for_class
            @inbounds X[features[f], classes[c]] += 1.0
        end
    end
    X
end

@timed X = count_vectorizer(x, y, ingredients, cuisines)

(
350x26 Array{Float64,2}:
 15.0   31.0   79.0  18.0  127.0   41.0  â€¦   11.0  56.0  20.0  397.0  17.0
 24.0   79.0   30.0  18.0    7.0   75.0      43.0   5.0   4.0  244.0  56.0
 21.0   35.0    7.0  14.0   19.0    7.0      20.0   2.0   4.0   97.0   2.0
  2.0   40.0  183.0  80.0  307.0   75.0     100.0  47.0  27.0   78.0  43.0
 30.0  125.0   70.0  21.0   38.0  125.0      40.0  84.0   3.0  345.0  88.0
 51.0   11.0   24.0  14.0   68.0    8.0  â€¦   10.0   2.0   6.0  228.0   0.0
 30.0   26.0   15.0   3.0    8.0   41.0       3.0   4.0   3.0  195.0  29.0
 12.0   18.0   31.0   0.0   12.0   13.0       1.0  14.0   1.0  142.0   2.0
  9.0    5.0    3.0   3.0   21.0    8.0       0.0   3.0   6.0   58.0   6.0
 18.0   21.0   12.0  40.0  108.0    7.0      21.0  11.0   8.0  145.0   6.0
 47.0  114.0  167.0  60.0  272.0  137.0  â€¦   63.0  89.0  19.0  565.0  67.0
 18.0    3.0    6.0   4.0   69.0    1.0       6.0   8.0   5.0   66.0   0.0
 11.0   20.0   15.0   8.0   23.0    3.0       4.0   2.0   8.0   92.0   2.0
  â‹®                                 â‹®    â‹±                             â‹®  
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0  â€¦    0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0  â€¦    0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   1.0   0.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   1.0    0.0   0.0
  0.0    0.0    0.0   0.0    0.0    0.0       0.0   0.0   0.0    1.0   0.0,

0.44882671,10309732)

print(sum(X, 1))
print(size(X))

[737.0 2804.0 2664.0 2510.0 8279.0 2184.0 1715.0 1957.0 759.0 1997.0 43541.0 1220.0 2330.0 1311.0 14436.0 685.0 5695.0 2177.0 504.0 2498.0 1160.0 1482.0 3055.0 1658.0 11644.0 1123.0](350,26)

function naive_bayes_fit(X::Array{Float64,2}, alpha::Float64 = 1.0)

    log_like = zeros(size(X))

    # sum over classes
    total = sum(X, 1)

    # sum over rows for each class for performance reasons
    for j in 1:size(X)[2]
        @inbounds log_like[:,j] = log(X[:,j] .+ alpha) .- log(total[j])
    end

    log_like
end

@timed log_like = naive_bayes_fit(X)

(
350x26 Array{Float64,2}:
 -3.83     -4.47307  -3.50556  -4.8836   â€¦  -4.36884  -3.37609  -4.13339
 -3.38371  -3.55678  -4.4536   -4.8836      -5.80393  -3.86129  -2.98071
 -3.51155  -4.35528  -5.80814  -5.11999     -5.80393  -4.77758  -5.92515
 -5.50398  -4.22523  -2.67265  -3.43359     -4.08116  -4.9931   -3.23957
 -3.1686   -3.10252  -3.6249   -4.737       -6.02707  -3.51611  -2.53512
 -2.65134  -5.4539   -4.66871  -5.11999  â€¦  -5.46746  -3.92882  -7.02376
 -3.1686   -4.64297  -5.115    -6.44174     -6.02707  -4.08443  -3.62256
 -4.03764  -4.99436  -4.42185  -7.82804     -6.72022  -4.3997   -5.92515
 -4.3      -6.14704  -6.50129  -6.44174     -5.46746  -5.28501  -5.07785
 -3.65815  -4.84776  -5.32263  -4.11447     -5.21614  -4.37894  -5.07785
 -2.73139  -3.19387  -2.76362  -3.71716  â€¦  -4.41764  -3.02395  -2.80425
 -3.65815  -6.55251  -5.94167  -6.2186      -5.62161  -5.15785  -7.02376
 -4.11768  -4.89428  -5.115    -5.63081     -5.21614  -4.82995  -5.92515
  â‹®                                      â‹±                       â‹®
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804  â€¦  -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804  â€¦  -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -6.72022  -9.36255  -7.02376
 -6.60259  -7.9388   -7.88758  -7.82804     -7.41337  -8.6694   -7.02376,

0.020244006,644824)

function naive_bayes_predict(predict::Array{String,1}, log_like::Array{Float64,2},
    prior::Array{Float64,1}, features::Dict{String,Int64}, rev_classes::Dict{Int64,String})

    results = zeros(length(rev_classes))

    for i in 1:length(rev_classes)
        ind = convert(Array{Int64,1}, [features[i] for i in predict]) # not sure why i have to do this
        results[i] = sum(log_like[ind, i]) + log(prior[i])
    end

    rev_classes[indmax(results)]
end

prior = ones(length(cuisines)) / length(cuisines)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Prediction&lt;/h3&gt;

&lt;p&gt;Now I can predict the cuisine given a list of ingredients.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# A prediction
i = 3
y[i], naive_bayes_predict(x[i], log_like, prior, ingredients, rev_cuisines), x[i]

(&amp;quot;Vietnamese&amp;quot;,&amp;quot;Vietnamese&amp;quot;,String[&amp;quot;garlic&amp;quot;,&amp;quot;soy_sauce&amp;quot;,&amp;quot;lime_juice&amp;quot;,&amp;quot;thai_pepper&amp;quot;])

# another one
i = 3000
y[i], naive_bayes_predict(x[i], log_like, prior, ingredients, rev_cuisines), x[i]

(&amp;quot;American&amp;quot;,&amp;quot;German&amp;quot;,String[&amp;quot;cane_molasses&amp;quot;,&amp;quot;butter&amp;quot;,&amp;quot;wheat&amp;quot;,&amp;quot;vanilla&amp;quot;,&amp;quot;ginger&amp;quot;,&amp;quot;honey&amp;quot;,&amp;quot;nutmeg&amp;quot;,&amp;quot;cinnamon&amp;quot;,&amp;quot;lard&amp;quot;,&amp;quot;egg&amp;quot;,&amp;quot;milk&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I can create a confusion matrix to see the overlap of the ingredients between
cuisines.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confusion_matrix = zeros(length(cuisines), length(cuisines))

for i in keys(cuisines)
    cuisine = x[find(_ -&amp;gt; _ == i, y)]
    total = length(cuisine)
    for j in cuisine
        predicted = naive_bayes_predict(j, log_like, prior, ingredients, rev_cuisines)
        confusion_matrix[cuisines[i], cuisines[predicted]] += 1 / total
    end
end

confusion_matrix

26x26 Array{Float64,2}:
 0.907692    0.0         0.0         â€¦  0.0         0.0         0.0
 0.0218978   0.627737    0.0            0.0328467   0.00364964  0.0255474
 0.0274914   0.00343643  0.42268        0.0824742   0.0         0.0652921
 0.00625     0.009375    0.0125         0.065625    0.0         0.021875  
 0.0100402   0.00401606  0.0291165      0.195783    0.00100402  0.0060241
 0.0207469   0.0165975   0.0746888   â€¦  0.0829876   0.00414938  0.190871  
 0.0205479   0.00684932  0.0            0.0616438   0.00684932  0.0273973
 0.365854    0.0304878   0.0            0.0         0.0304878   0.00609756
 0.0         0.0108696   0.0            0.0869565   0.0         0.0
 0.0         0.00444444  0.00444444     0.0311111   0.0         0.00444444
 0.0280674   0.0114274   0.0252606   â€¦  0.152165    0.00240577  0.0643545
 0.026087    0.0521739   0.0173913      0.0347826   0.0         0.00869565
 0.00806452  0.0322581   0.0201613      0.0322581   0.0         0.016129  
 0.00684932  0.00684932  0.0205479      0.143836    0.0         0.0273973
 0.00116618  0.00116618  0.0268222      0.0874636   0.0         0.0110787
 0.0         0.0         0.0         â€¦  0.0930233   0.0         0.0
 0.0369775   0.00803859  0.0144695      0.0498392   0.0         0.302251  
 0.039823    0.00884956  0.0            0.00884956  0.0265487   0.00442478
 0.0         0.0         0.0            0.115385    0.0         0.0192308
 0.0         0.0103806   0.0484429      0.0484429   0.0         0.0138408
 0.0441176   0.00735294  0.0         â€¦  0.0220588   0.0147059   0.0
 0.00729927  0.0364964   0.0145985      0.0145985   0.0         0.00729927
 0.00867052  0.00289017  0.0144509      0.106936    0.0         0.0606936
 0.00980392  0.00980392  0.0196078      0.485294    0.0         0.00490196
 0.182823    0.0280612   0.0            0.0102041   0.0637755   0.00935374
 0.00925926  0.00925926  0.0185185   â€¦  0.00925926  0.0185185   0.685185  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What is the cuisine that is easiest to predict? Vietnamese.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confuse = diag(confusion_matrix)
sort([(rev_cuisines[i], confuse[i]) for i in 1:length(cuisines)])

26-element Array{(Any,Any),1}:
 (&amp;quot;African&amp;quot;,0.32173913043478275)
 (&amp;quot;American&amp;quot;,0.00541299117882919)
 (&amp;quot;Asian&amp;quot;,0.06377551020408168)
 (&amp;quot;Cajun_Creole&amp;quot;,0.7054794520547963)
 (&amp;quot;Central_SouthAmerican&amp;quot;,0.2531120331950211)
 (&amp;quot;Chinese&amp;quot;,0.756637168141594)
 (&amp;quot;EasternEuropean_Russian&amp;quot;,0.26027397260273977)
 (&amp;quot;English_Scottish&amp;quot;,0.48529411764705815)
 (&amp;quot;French&amp;quot;,0.14156626506024103)
 (&amp;quot;German&amp;quot;,0.7692307692307696)
 (&amp;quot;Greek&amp;quot;,0.6088888888888881)
 (&amp;quot;Indian&amp;quot;,0.6277372262773735)
 (&amp;quot;Irish&amp;quot;,0.6627906976744192)
 (&amp;quot;Italian&amp;quot;,0.4740524781341024)
 (&amp;quot;Japanese&amp;quot;,0.7794117647058805)
 (&amp;quot;Jewish&amp;quot;,0.3406249999999993)
 (&amp;quot;Mediterranean&amp;quot;,0.33217993079584757)
 (&amp;quot;Mexican&amp;quot;,0.29099678456591677)
 (&amp;quot;MiddleEastern&amp;quot;,0.3225806451612901)
 (&amp;quot;Moroccan&amp;quot;,0.6131386861313863)
 (&amp;quot;Scandinavian&amp;quot;,0.6304347826086959)
 (&amp;quot;Southern_SoulFood&amp;quot;,0.3930635838150301)
 (&amp;quot;Southwestern&amp;quot;,0.6851851851851855)
 (&amp;quot;Spanish_Portuguese&amp;quot;,0.422680412371133)
 (&amp;quot;Thai&amp;quot;,0.4878048780487808)
 (&amp;quot;Vietnamese&amp;quot;,0.9076923076923062)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like &amp;ldquo;Asian&amp;rdquo; cuisine is so general that the classifier cannot predict
correctly. Let&amp;rsquo;s see what it&amp;rsquo;s confused with.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confuse = confusion_matrix[cuisines[&amp;quot;Asian&amp;quot;],:]
[(rev_cuisines[i], confuse[i]) for i in 1:length(cuisines)]

26-element Array{(Any,Any),1}:
 (&amp;quot;Vietnamese&amp;quot;,0.18282312925170086)
 (&amp;quot;Indian&amp;quot;,0.028061224489795925)
 (&amp;quot;Spanish_Portuguese&amp;quot;,0.0)
 (&amp;quot;Jewish&amp;quot;,0.004251700680272108)
 (&amp;quot;French&amp;quot;,0.002551020408163265)
 (&amp;quot;Central_SouthAmerican&amp;quot;,0.0008503401360544217)  
 (&amp;quot;Cajun_Creole&amp;quot;,0.005952380952380952)
 (&amp;quot;Thai&amp;quot;,0.10034013605442185)
 (&amp;quot;Scandinavian&amp;quot;,0.006802721088435374)
 (&amp;quot;Greek&amp;quot;,0.002551020408163265)
 (&amp;quot;American&amp;quot;,0.0008503401360544217)
 (&amp;quot;African&amp;quot;,0.006802721088435374)
 (&amp;quot;MiddleEastern&amp;quot;,0.0017006802721088435)
 (&amp;quot;EasternEuropean_Russian&amp;quot;,0.0008503401360544217)
 (&amp;quot;Italian&amp;quot;,0.002551020408163265)
 (&amp;quot;Irish&amp;quot;,0.003401360544217687)
 (&amp;quot;Mexican&amp;quot;,0.002551020408163265)
 (&amp;quot;Chinese&amp;quot;,0.31037414965986226)
 (&amp;quot;German&amp;quot;,0.028911564625850348)
 (&amp;quot;Mediterranean&amp;quot;,0.0)
 (&amp;quot;Japanese&amp;quot;,0.2176870748299322)
 (&amp;quot;Moroccan&amp;quot;,0.00510204081632653)
 (&amp;quot;Southern_SoulFood&amp;quot;,0.0017006802721088435)
 (&amp;quot;English_Scottish&amp;quot;,0.010204081632653059)
 (&amp;quot;Asian&amp;quot;,0.06377551020408168)
 (&amp;quot;Southwestern&amp;quot;,0.009353741496598638)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yup, &amp;ldquo;Asian&amp;rdquo; is confused with other Asian cuisines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparing Crowdsourced and Government Fukushima Radiation Data</title>
      <link>http://tokestermw.github.io/posts/fukushima-data</link>
      <pubDate>Thu, 21 Nov 2013 00:00:00 UTC</pubDate>
      
      <guid>http://tokestermw.github.io/posts/fukushima-data</guid>
      <description>

&lt;h2 id=&#34;toc_0&#34;&gt;Mapping the Difference between Crowdsourced and Government Data&lt;/h2&gt;

&lt;p&gt;I decided it might be a good exercise to actually check between the two datasets. My main objective is to create an app that plots both data, interpolate the data for direct comparison and quantify the difference. Since the data are high resolution both in time and space, I thought it was best to let the user get a feel of what the difference were.&lt;/p&gt;

&lt;p&gt;The government data is 23+ static points around Fukushima Daiichi with radiation measured every 10 minutes. The crowdsourced data is measured using sensors attached to volunteers&amp;rsquo; cars. Therefore, I did an interpolation on the government data but not the crowdsourced data for the comparison.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Interpolation Methods&lt;/h3&gt;

&lt;p&gt;I used three different interpolation methods. I did this to reduce model error and to not get accused of cherry-picking models to fail to reject my null (H0: Government = Crowdsourced).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inverse Distance Weighting&lt;/em&gt;: This is the simplest model. Farther off points are less related to the interpolated point than closer ones. I could have also used k-nearest neighbors but that was overkill for a basic model to compare against.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Radial Basis Network&lt;/em&gt;: RBNs are a type of neural network that takes radial activation functions (Gaussian in my case, I&amp;rsquo;ve also looked at splines) at each data point and and fits linear weights to get the interpolated point. Since I&amp;rsquo;m interpolating, the centers of the radial functions are known so back propagation is not needed. Linear weights are fit using the matrix inverse (&lt;a href=&#34;http://math.bu.edu/people/mkon/nnpap3.pdf&#34;&gt;ref&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ordinary Kriging&lt;/em&gt;: I can estimate the linear weights by minimizing the MSE. This breaks up into variance, covariance and semivariance so I fit the variogram, then run the interpolation. Since kriging assumes a normal random field, a confidence interval can be calculated. Looking ahead, ordinary kriging assumes the same mean for all points so universal kriging may be more apropos.&lt;/p&gt;

&lt;p&gt;I fit and train them offline to make it ready for the interactive app.&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Comparison&lt;/h3&gt;

&lt;p&gt;The comparison between the two datasets is simple. I do a log on both datasets, and then for each point from the crowdsourced data, I run the interpolation on the government data for that location. I take a ratio of these points and plot them on a map.&lt;/p&gt;

&lt;p&gt;To compare between the interpolation methods, I considered leave one out cross-validation MSEs, K-fold cross-validation with respect to latitude and ditto with longitude. I also considered interpolated points that are closer to the crowdsourced points as &amp;ldquo;better.&amp;rdquo; Generally, the kriging method performed the best.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Map App&lt;/h3&gt;

&lt;p&gt;The app (&lt;a href=&#34;http://54.200.81.28:5000/&#34;&gt;here&lt;/a&gt;[link broken]) allows users to play with the data and its comparisons. The user can choose the date, choose the model, choose the CPM-to-ÂµSv/hr conversion rate and choose the &amp;ldquo;percentage difference.&amp;rdquo; The conversion rate &lt;a href=&#34;http://blog.safecast.org/2011/06/volunteer-report-safecasting-miyagi-by-rob-kneller/&#34;&gt;defaults to 300&lt;/a&gt; so 300 CPM is equivalent to 1ÂµSv/hr. The &amp;ldquo;percentage difference&amp;rdquo; just lets you choose how far apart the two values need to be to consider it significant. So choosing 10% means that if the citizen data is within +/-10% of the government data, the marker on the map is set to &lt;font color=&#34;green&#34;&gt;green&lt;/font&gt;.&lt;/p&gt;

&lt;p&gt;Here is a sample output from 09-24-2011 (kriging, 300, 10%):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../..//images/ok.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;The &lt;font color=&#34;red&#34;&gt;red markers&lt;/font&gt; are when the government data is bigger than the crowdsourced data, the &lt;font color=&#34;blue&#34;&gt;blue markers&lt;/font&gt; are when the crowdsourced data is bigger and the &lt;font color=&#34;green&#34;&gt;green markers&lt;/font&gt; shows anything between +/-10% of each other.&lt;/p&gt;

&lt;p&gt;So the results look pretty good. You can check for other dates but the kriging method approximates the crowdsourced data pretty well. The data diverge most around the epicenter of where the meltdown was. This isn&amp;rsquo;t too surprising since it&amp;rsquo;s where the radiation is the highest and most variable. For this date, the median ratio between crowdsourced and government data was ~98%.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Unit Conversion&lt;/h3&gt;

&lt;p&gt;A very important detail is the unit conversion rate. I defaulted to 300, but since it&amp;rsquo;s measuring different things (electron count vs. radiation exposure), there is no perfect conversion. The conversion may be different depending on sensor, location, wind etc. So below is a plot of median crowdsourced to government ratio for different conversion rates:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/conversion_profile.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;At least for 09-24-2013, the optimal conversion rate is 270. Whether that is the best conversion it&amp;rsquo;s not possible to know given this data.&lt;/p&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;No clear difference&lt;/h3&gt;

&lt;p&gt;For this set of data, I found no telling difference. Interpolation does not work well outside the interpolated points by definition and the results become more unstable as you get closer to the epicenter. All in all, it&amp;rsquo;s good to have multiple datasets as checks on the system&lt;/p&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sendung.de/japan-radiation-open-data/&#34;&gt;Link to Government Data&lt;/a&gt;, &lt;a href=&#34;http://blog.safecast.org/&#34;&gt;Link to Crowdsourced Data&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tokestermw/fukushima-nuclear-radiation&#34;&gt;GitHub Repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://54.200.81.28:5000/&#34;&gt;App&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Shrinkage and Election Polling</title>
      <link>http://tokestermw.github.io/posts/shrinkage-public-polling</link>
      <pubDate>Sat, 26 Oct 2013 00:00:00 UTC</pubDate>
      
      <guid>http://tokestermw.github.io/posts/shrinkage-public-polling</guid>
      <description>

&lt;p&gt;We saw in 2012 that this type of meta-analysis estimate can be done by anyone who is trained in advanced statistics. &lt;a href=&#34;http://votamatic.org/about-me/&#34;&gt;Drew Linzer&lt;/a&gt;, &lt;a href=&#34;http://election.princeton.edu/&#34;&gt;Sam Wang&lt;/a&gt; and &lt;a href=&#34;http://jackman.stanford.edu/blog/&#34;&gt;Simon Jackman&lt;/a&gt; all had their models predict the election outcome very accurately. All models essentially have the same underlying state polling data and some kind of averaging methodology for the overall voter preference.&lt;/p&gt;

&lt;p&gt;The averaging logic comes from the &lt;a href=&#34;http://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator&#34;&gt;James-Stein estimator&lt;/a&gt;. The James-Stein estimator basically says that the maximum likelihood estimate performs better on average when combined with the overall average. In other words, if the MLE shares information, or &amp;ldquo;shrunk&amp;rdquo; to the overall average, we get a more reliable estimate. The James-Stein estimator is just the weighted average of the MLE and the overall average.&lt;/p&gt;

&lt;p&gt;$$ \hat{y} = \bar{y} + C (\hat{y}_{MLE} - \bar{y} ) $$&lt;/p&gt;

&lt;p&gt;This logic can be extended to hierarchical models and Bayesian models where several probability distributions are combined. In the political polling context, we can greatly increase the sample size and precision without sacrificing too much bias.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Hierarchical Forward Random Walk Model for the 2012 Presidential Election&lt;/h2&gt;

&lt;p&gt;I decided to explore shrinkage using data from the last election. I ran &lt;a href=&#34;http://www.tandfonline.com/doi/abs/10.1080/10361140500302472#preview&#34;&gt;Simon Jackman&amp;rsquo;s hierarchical model&lt;/a&gt; on &lt;a href=&#34;http://elections.huffingtonpost.com/pollster/2012-general-election-romney-vs-obama&#34;&gt;national polling
data&lt;/a&gt;. To simplify, I aggregated the data into weeks (instead of days) and looked at hierarchical only at the pollster level. The other folks mentioned above who were doing meta-analysis are more careful with their analysis by including informative priors, running their models on state polling data, constructing a more sophisticated time series model, etc. The statistic of interest is the proportion of Obama support relative to Romney support. The model predicting anything above 50% suggests an Obama win.&lt;/p&gt;

&lt;p&gt;The model is simple. I fit a Binomial,&lt;/p&gt;

&lt;p&gt;$$y_i \sim \text{Binomial}(p_i, N_i)$$&lt;/p&gt;

&lt;p&gt;the expected value is split into a daily component and a pollster-level component,&lt;/p&gt;

&lt;p&gt;$$\text{logit}(p_i) = \alpha_{t_i} + \delta_{j_i}$$&lt;/p&gt;

&lt;p&gt;the daily component is filtered with a random walk model,&lt;/p&gt;

&lt;p&gt;$$\alpha_t \sim N(\alpha_{t-1}, \sigma^2_\alpha)$$&lt;/p&gt;

&lt;p&gt;where the spread of pollster effects are normal,&lt;/p&gt;

&lt;p&gt;$$\delta_j \sim N(0,\sigma^2_\delta)$$&lt;/p&gt;

&lt;p&gt;and it is assumed that the overall pollster effect evens out.&lt;/p&gt;

&lt;p&gt;$$\delta_1 = 0 - \sum^J_{j=2} \delta_j$$&lt;/p&gt;

&lt;p&gt;The model is fit with &lt;a href=&#34;http://mcmc-jags.sourceforge.net/&#34;&gt;JAGS&lt;/a&gt;. The model isn&amp;rsquo;t very dynamic in the sense that it doesn&amp;rsquo;t adjust for the end of the election cycle polling rush. But it does predict the right candidate. The fit of the model is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/model-average.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;The pollster effect can be quantified and is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/house-bias.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Since the election is over, the more interesting part is the shrinkage. I calculated the James-Stein estimator for all $p_i$ and compared it against the original polling data. Even with the drift over time, we see shrinkage towards the final number.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/shrinkage.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;I decided to focus on the biggest pollsters. The biggest pollsters are the ones that get cited by media. Below I plotted on the left, all the results from YouGov and Rasmussen along with the simple average of all the polls during that week (black line). On the right hand side I put the final poll and its sampling error, compared with the hierarchical model estimate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../images/pooled-estimate.png&#34; alt=&#34;bla&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The JAGS code:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model

## Pooling the Polls Over an Election Campaign
## SIMON JACKMAN

{
  ## random walk
  for (t in 2:T) {
    alpha[t] ~ dnorm(alpha[t-1], tau_alpha)
  }

  ## not a time series, fit to polling data
  for (i in 1:I) {
    logit(p[i]) &amp;lt;- alpha[id.t[i]] + delta[id.j[i]]
    y[i] ~ dbin(p[i], N[i])
  }

  ## sum to zero constraint
  delta[1] &amp;lt;- 0 - sum(delta[2:J])

  ## priors
  alpha[1] ~ dnorm(0, .1)
  for (j in 2:J) {
    delta[j] ~ dnorm(0, tau_delta)
  }

  sigma_delta ~ dunif(0, 100)
  tau_delta &amp;lt;- 1 / pow(sigma_delta, 2)

  sigma_alpha ~ dunif(0, 100)
  tau_alpha &amp;lt;- 1 / pow(sigma_alpha, 2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Refs:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Simon Jackman, Pooling the Polls Over an Election Campaign. Australian Journal of Political Science, December 2005, Vol 40, No. 4, pp. 499-517.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>D3 and jstat to visualize statistics</title>
      <link>http://tokestermw.github.io/posts/d3-with-jstat</link>
      <pubDate>Sat, 21 Sep 2013 00:00:00 UTC</pubDate>
      
      <guid>http://tokestermw.github.io/posts/d3-with-jstat</guid>
      <description>&lt;p&gt;R does plotting statistics well, D3.js does plotting web-friendly interactive plots well. So I decided to weave &lt;a href=&#34;http://www.jstat.org/&#34;&gt;jstat&lt;/a&gt; (a Javascript stats library) into D3 to plot R-like objects. One of the things people during data analysis is to plot theoretical distributions to check it against the data. In jstat, you can easily do this by using the normal pdf function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var range = jStat(jStat.seq(-5,5,100));
var densities = range.normal(0.0,1.0).pdf()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can feed this to a &amp;ldquo;data frame&amp;rdquo; by using normal Javascript.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var dataset = [], zerodata = [];
for (var i = 0; i &amp;lt; range[0].length; i++) {
    dataset.push([range[0][i], densities[0][i]]);
    zerodata.push([0, 0]);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The best feature in D3 is its interpolating algorithm to go from one shape to another. You can create a &lt;code&gt;d3.svg.area&lt;/code&gt; object to define an area from the bottom y = 0, to the top of the distribution.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:::javascript
var pdfArea = d3.svg.area()
    .x(function(d) { return xscale(d[0]) })
    .y1(function(d) { return yscale(d[1]) })
    .y0(h)
    .interpolate(&amp;quot;basis&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the populated data frame from above, you can append a &lt;code&gt;path&lt;/code&gt; going from &lt;code&gt;zerodata&lt;/code&gt; (at y = 0) to &lt;code&gt;dataset&lt;/code&gt; (at y = dnorm(x)).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;svg.append(&amp;quot;path&amp;quot;)
    .attr(&amp;quot;d&amp;quot;, pdfArea(zerodata))
    .attr(&amp;quot;fill&amp;quot;, &amp;quot;green&amp;quot;)
    .transition().delay(500).duration(1000)
    .attr(&amp;quot;d&amp;quot;, pdfArea(dataset))
    .attr(&amp;quot;fill&amp;quot;, d3.rgb(255, 124, 138));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add some more features like clicking.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;svg.on(&amp;quot;click&amp;quot;, function() {
    newmean = Math.random() * 6 - 3;
    newden = range.normal(newmean,1.0).pdf()
    var newdata = [];
    for (var i = 0; i &amp;lt; range[0].length; i++) {
        newdata.push([range[0][i], newden[0][i]]);
    }
    svg.select(&amp;quot;path&amp;quot;)
        .attr(&amp;quot;d&amp;quot;, pdfArea(zerodata))
        .attr(&amp;quot;fill&amp;quot;, &amp;quot;pink&amp;quot;)
        .transition().delay(500).duration(1000)
        .attr(&amp;quot;d&amp;quot;, pdfArea(newdata))
        .attr(&amp;quot;fill&amp;quot;, d3.rgb(255, 124, 138));
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final result should be in the sidebar.&lt;/p&gt;

&lt;p&gt;Entire code is posted in &lt;a href=&#34;http://bl.ocks.org/tokestermw/6652994&#34;&gt;bl.ocks&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>