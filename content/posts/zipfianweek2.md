title: Zipfian Academy Week 2 log: Naive Bayesian Learning
date: 2013-09-28
tags: zipfian academy
author: Motoki Wu
summary: theorize in a quiet place without the coffee
status: draft

This week we went deeper into Python OO principles (classes, subclasses, scoping, instances, inheritance, etc.), packages for analytics (mainly NumPy, SciPy, pandas, and PyMC) and visualizations (D3, matplotlib and [processing](http://processing.org/)). We also talked a bit on data structures and algorithmic efficiencies (more for job interviews). There are more networking events than I expected as there are a ton of [data meetups](http://www.meetup.com/find/?keywords=data+science&radius=25&userFreeform=san+francisco%2C+ca&gcResults=San+Francisco%2C+CA%2C+USA%3AUS%3ACA%3ASan+Francisco%3ASan+Francisco%3A%3A%3A37.7749295%3A-122.4194155&sort=default) around SF. Much of next week will be taken by [DataWeek](http://dataweek.co/) where a few of us will participate in a hackathon. 

We actually touched Bayesian statistics, MCMC and frequentist hypothesis testing in basically one day. As a person who studied statistics in college (plus two years of graduate-level stats), it confirmed how impossible it is to learn all of this stuff in compressed fashion. It was a breeze for me (PyMC is **really** similar to WinBUGS / JAGS), but I can only imagine that it was a blur for people who haven't been exposed to deeper statistics. Similar to when you do have a job, the practical skills can be learned very quickly in an environment where you're working on tangible projects with real deadlines. 

So for reference I decided

1. kdjf
    1. dkfjd kfjs df
    2. dkfj


Next week, we'll be delving into machine learning which I'm not too familiar with. Expect me to have the [MacKay book](http://www.inference.phy.cam.ac.uk/itila/p0.html) and the [Hastie book](http://www-stat.stanford.edu/~tibs/ElemStatLearn/) at all times.